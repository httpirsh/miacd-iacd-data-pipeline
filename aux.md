dps apagar --

1. Kafka Producer
L√™ um dataset de emiss√µes de CO2 (ficheiro CSV)

Envia cada linha do dataset para um t√≥pico Kafka (emissions-topic) em formato JSON

O producer est√° a funcionar e a enviar dados continuamente (em loop)

2. Spark Streaming
Consome os dados do t√≥pico Kafka

Processa os dados em batches a cada 30 segundos

No c√≥digo atual, est√° a fazer uma an√°lise simples (debug) que inclui:

Contar o n√∫mero de registos

Mostrar algumas estat√≠sticas (m√©dia de CO2, etc.)

Mostrar os top 10 emissores de CO2

3. PostgreSQL
Configuramos uma base de dados PostgreSQL para armazenar os resultados do clustering

Criamos duas tabelas:

co2_clusters: para armazenar os resultados do clustering por pa√≠s

cluster_stats: para armazenar estat√≠sticas agregadas por cluster

Tamb√©m criamos uma view cluster_analysis para facilitar a an√°lise

Inserimos um registo de exemplo para teste


---

Tabela co2_clusters
- id: Chave prim√°ria
- batch_id: ID do batch de processamento (√∫til para acompanhar o streaming)
- country: Nome do pa√≠s
- iso_code: C√≥digo ISO do pa√≠s
- avg_co2: M√©dia de emiss√µes de CO2 (em milh√µes de toneladas) para o pa√≠s no per√≠odo analisado
- avg_co2_per_capita: M√©dia de emiss√µes per capita
- avg_gdp: M√©dia do PIB
- avg_population: M√©dia da popula√ß√£o
- cluster: Cluster atribu√≠do pelo algoritmo K-means
- processing_time: Timestamp do processamento

Tabela cluster_stats
- id: Chave prim√°ria
- batch_id: ID do batch
- cluster: N√∫mero do cluster
- num_countries: N√∫mero de pa√≠ses no cluster
- avg_co2_cluster: M√©dia de CO2 do cluster
- avg_co2_per_capita_cluster: M√©dia de CO2 per capita do cluster
- avg_gdp_cluster: M√©dia do PIB do cluster
- processing_time: Timestamp

View cluster_analysis
- Agrega os dados da tabela co2_clusters por cluster, mostrando:
- N√∫mero de pa√≠ses
- M√©dias de CO2, CO2 per capita e PIB

---
Como verificar o estado atual?
Kafka Producer: docker-compose logs producer - deve mostrar os pa√≠ses a serem enviados

Spark: docker-compose logs spark - deve mostrar os batches a serem processados e as estat√≠sticas

PostgreSQL: Pode usar o pgAdmin (http://localhost:5050) ou o comando psql para ver as tabelas

---

Dataset CO‚ÇÇ ‚Üí Kafka Producer ‚Üí Kafka Broker ‚Üí Spark Streaming ‚Üí PostgreSQL ‚Üí (Pr√≥ximo: Superset)

---

objetivo do projeto:
Criar um pipeline de dados em tempo real para:
- Analisar emiss√µes de CO‚ÇÇ por pa√≠s
- Agrupar pa√≠ses por padr√µes de emiss√µes (clustering)
- Visualizar os resultados em dashboards

---
üí° Para Saber Mais:
Documenta√ß√£o PostgreSQL: https://www.postgresql.org/docs/

Spark + PostgreSQL: https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html

Docker Compose: https://docs.docker.com/compose/

