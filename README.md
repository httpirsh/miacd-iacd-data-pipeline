# CO2 Data Engineering Pipeline

A complete data engineering pipeline for analyzing global CO2 emissions using **Kafka, Spark, PostgreSQL, and Superset** deployed on **Kubernetes**.

---

## ğŸ“Š Project Overview

### Dataset
**Source:** Our World in Data - CO2 Emissions Dataset

| Aspect | Original | Processed |
|--------|----------|-----------|
| **Rows** | 50,407 | 23,405 (1900-2014 filtered) |
| **Columns** | 79 | 7 (selected variables) |
| **Size** | 14MB | 1.4MB |
| **Time Range** | 1750-2014 | 1900-2014 (115 years) |
| **Data Quality** | Sparse pre-1900 | Dense, complete |

### Selected Variables (7 columns)
1. `country` - Country name
2. `year` - Year (1900-2014)
3. `iso_code` - ISO 3-letter country code
4. `population` - Population
5. `gdp` - Gross Domestic Product
6. `co2` - Total CO2 emissions (million tonnes)
7. `co2_per_capita` - Per capita emissions (tonnes)

**Rationale**: Focused on time-series analysis, country comparisons, GDP-emissions correlation, and geographic visualizations. Pre-1900 data was removed due to significant gaps and missing values.

---

## ğŸ—ï¸ Architecture

### Components
1. **Apache Kafka** (KRaft mode) - Message broker for streaming
2. **Apache Spark** (Master + Worker) - Data processing engine
3. **PostgreSQL** - Relational database for storage
4. **Apache Superset** - Data visualization dashboards

### Data Flow
```
CSV (23K rows, 7 cols) â†’ Kafka Producer â†’ Kafka Topic (co2-raw)
                              â†“
                        Spark Consumer â†’ PostgreSQL (raw_emissions table)
                              â†“
                        Superset Dashboards
```

---

## ğŸ“‚ Database Schema

### Tables
1. **raw_emissions** (7 columns)
   - All raw data: country, year, iso_code, population, gdp, co2, co2_per_capita
   
2. **country_summary** (aggregated by country)
   - Total CO2, avg per capita, rankings, latest year data
   
3. **yearly_summary** (aggregated by year)
   - Global totals, growth rates, country counts

### Views
- **top_polluters** - Top 10 countries by total CO2
- **top_per_capita** - Top 10 by per capita emissions
- **recent_trends** - Last 20 years of global data

---

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ README.md                      # This file - Project overview
â”œâ”€â”€ STATUS.md                      # Current deployment status & next steps
â”œâ”€â”€ ANALYSIS.md                    # Complete data analysis and insights
â”œâ”€â”€ SUPERSET_SETUP.md              # Superset dashboard setup guide
â”œâ”€â”€ DASHBOARD_QUICK_START.md       # Quick reference for charts
â”œâ”€â”€ aux.md                         # Additional notes
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ docker-compose.yml             # Docker Compose (alternative to K8s)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_reduced.py         # Dataset extraction & filtering script
â”‚   â””â”€â”€ eda.ipynb                  # Exploratory Data Analysis notebook
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py                # Kafka producer (â†’ co2-raw topic)
â”‚   â”œâ”€â”€ Dockerfile                 # Kafka producer container
â”‚   â””â”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ consumer.py                # Kafka â†’ PostgreSQL streaming
â”‚   â”œâ”€â”€ Dockerfile                 # Spark consumer container
â”‚   â””â”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ init.sql                   # Database schema (3 tables, 3 views)
â”‚   â””â”€â”€ info.txt                   # Connection information
â””â”€â”€ kubernetes/                    # Kubernetes deployment manifests
    â”œâ”€â”€ 01-postgres-pvc.yaml       # PostgreSQL storage
    â”œâ”€â”€ 02-postgres-deploy.yaml    # PostgreSQL deployment
    â”œâ”€â”€ 03-postgres-service.yaml   # PostgreSQL service
    â”œâ”€â”€ 04-kafka-kraft.yaml        # Kafka in KRaft mode
    â”œâ”€â”€ 05-spark-master.yaml       # Spark master
    â”œâ”€â”€ 06-spark-worker.yaml       # Spark worker
    â””â”€â”€ 07-superset.yaml           # Superset dashboard
```

**Note on Data Files:**
- The `data/` directory contains the dataset files and is excluded from git (see `.gitignore`)
- Original dataset: [Our World in Data - CO2 Emissions](https://github.com/owid/co2-data)
- Download `owid-co2-data.csv` and use `scripts/extract_reduced.py` to generate the reduced dataset
- `reduced_co2.csv`: 23,405 rows Ã— 7 columns (1900-2014, filtered)


---

## ğŸš€ Quick Start (Kubernetes/Minikube)

### Prerequisites
- Minikube
- kubectl
- Python 3.8+

### 1. Start Minikube
```bash
minikube start --cpus=2 --memory=4096
```

### 2. Deploy all components
```bash
cd kubernetes
kubectl apply -f 01-postgres-pvc.yaml
kubectl apply -f 02-postgres-deploy.yaml
kubectl apply -f 03-postgres-service.yaml
kubectl apply -f 04-kafka-kraft.yaml
kubectl apply -f 05-spark-master.yaml
kubectl apply -f 06-spark-worker.yaml
kubectl apply -f 07-superset.yaml
# Or apply all at once:
# kubectl apply -f kubernetes/
```

### 3. Wait for pods to be ready
```bash
kubectl get pods -w
# Wait until all pods show "Running"
```

### 4. Initialize PostgreSQL schema
```bash
# Get the PostgreSQL pod name
POSTGRES_POD=$(kubectl get pods -l app=postgres -o jsonpath='{.items[0].metadata.name}')

# Copy the init.sql file to the pod
kubectl cp postgres/init.sql $POSTGRES_POD:/tmp/init.sql

# Execute the schema
kubectl exec -it $POSTGRES_POD -- psql -U co2_user -d co2_data -f /tmp/init.sql
```

### 5. Create Kafka topic
```bash
kubectl exec -it kafka-0 -- kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --create --topic co2-raw \
  --partitions 3 --replication-factor 1
```

### 6. Load data via Kafka streaming
```bash
# The Kafka producer reads from data/reduced_co2.csv (not included in repo)
# Download the dataset first:
# wget https://github.com/owid/co2-data/raw/master/owid-co2-data.csv -O data/owid-co2-data.csv

# Create data directory if it doesn't exist
mkdir -p data/

# Run the extraction script to create reduced_co2.csv
python scripts/extract_reduced.py

# Port-forward Kafka to localhost
kubectl port-forward kafka-0 9092:9092 &

# Run the Kafka producer (from local machine)
cd kafka
pip install -r requirements.txt
python producer.py

# In another terminal, submit the Spark consumer
SPARK_POD=$(kubectl get pods -l app=spark-master -o jsonpath='{.items[0].metadata.name}')
kubectl cp consumer.py $SPARK_POD:/tmp/
kubectl exec -it $SPARK_POD -- spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.0 \
  /tmp/consumer.py
```

### 7. Access Superset
```bash
kubectl port-forward svc/superset 8088:8088
# Open http://localhost:8088
# Login: admin/admin
```

**Add PostgreSQL connection in Superset:**
- Host: `postgres.default.svc.cluster.local`
- Port: `5432`
- Database: `co2_data`
- Username: `co2_user`
- Password: `co2_password`

## ğŸ“Š Database Schema

### Tables (7 columns matching CSV)
1. **raw_emissions** - All data from Kafka stream
   - country, year, iso_code, population, gdp, co2, co2_per_capita
2. **country_summary** - Aggregated by country (totals, rankings)
3. **yearly_summary** - Aggregated by year (global trends)

### Views
- `top_polluters` - Top 10 countries by total CO2
- `top_per_capita` - Top 10 by per capita emissions
- `recent_trends` - Last 20 years

## ğŸ”§ Useful Commands (Kubernetes)

### Check pod status
```bash
kubectl get pods
kubectl describe pod <pod-name>
kubectl logs <pod-name>
```

### Create Kafka topic
```bash
kubectl exec -it kafka-0 -- kafka-topics.sh \
  --list \
  --bootstrap-server localhost:9092
```

### Monitor Kafka messages
```bash
kubectl exec -it kafka-0 -- kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic co2-raw \
  --from-beginning \
  --max-messages 10
```

### Connect to PostgreSQL
```bash
kubectl port-forward postgres-0 5432:5432
PGPASSWORD=co2_password psql -h localhost -U co2_user -d co2_data
```

### Query data
```sql
-- Count records
SELECT COUNT(*) FROM raw_emissions;

-- Top 10 polluters
SELECT * FROM top_polluters;

-- Recent global trends
SELECT * FROM recent_trends;
```

### Check Spark jobs
```bash
# Port-forward Spark Master UI
kubectl port-forward svc/spark-master 8080:8080
# Open http://localhost:8080
```

## ğŸ“ˆ Superset Dashboards

### Dashboard Ideas
1. **Global Overview**
   - Line chart: Global CO2 over time
   - Area chart: Emission sources (coal, oil, gas)

2. **Country Rankings**
   - Bar chart: Top 10 total polluters
   - Bar chart: Top 10 per capita
   - Table: All countries ranked

3. **Trends Analysis**
   - Scatter: GDP vs CO2
   - Line chart: YoY growth rates
   - Heatmap: Regional emissions

## ğŸ§ª Testing

### Test producer only
```bash
python kafka/producer.py --max-records 100
```

### Test database connection
```bash
docker exec -it postgres psql -U co2_user -d co2_data -c "SELECT COUNT(*) FROM raw_emissions;"
```

### Test Spark locally
```bash
cd spark
pip install -r requirements.txt
python aggregation_job.py
```

## ğŸ“ Development Workflow

1. **Week 1**: Set up Docker Compose environment
2. **Week 2**: Implement and test Kafka producer/consumer
3. **Week 3**: Develop Spark processing jobs
4. **Week 4**: Create Superset dashboards
5. **Week 5**: Deploy to Kubernetes and test

## ğŸ› ï¸ Troubleshooting

### Kafka not receiving messages
- Check if topic exists: `kubectl exec -it kafka-0 -- kafka-topics.sh --list --bootstrap-server localhost:9092`
- Check producer logs for errors
- Verify port-forward is active: `kubectl port-forward kafka-0 9092:9092`

### Spark consumer not writing to PostgreSQL
- Check PostgreSQL is running: `kubectl get pods | grep postgres`
- Verify JDBC URL uses internal service name: `postgres.default.svc.cluster.local`
- Check Spark logs: `kubectl logs <spark-master-pod>`

### Superset can't connect to PostgreSQL
- Use hostname `postgres.default.svc.cluster.local` (not localhost)
- Verify credentials: co2_user/co2_password
- Check if PostgreSQL pod is running

### Out of memory errors
- Check Minikube resources: `minikube config get memory`
- Reduce Spark worker replicas or memory requests
- Current setup uses 512Mi per Spark pod

### Pod stuck in CrashLoopBackOff
- Check logs: `kubectl logs <pod-name>`
- Describe pod: `kubectl describe pod <pod-name>`
- Verify image versions are correct (apache/kafka:4.1.0, apache/spark:4.0.1)

## ğŸ¯ Success Criteria

- âœ… 23K+ records loaded from reduced CSV
- âœ… Data cleaned and transformed by Spark
- âœ… PostgreSQL contains all 3 tables with data
- âœ… Superset displays interactive dashboards
- âœ… All 5 components running on Kubernetes
- âœ… Pipeline processes data efficiently

## ğŸ“š Resources

- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Superset Documentation](https://superset.apache.org/docs/intro)
- [Dataset Source](https://github.com/owid/co2-data)
- [Kubernetes Documentation](https://kubernetes.io/docs/home/)

---

## ï¿½ Implementation Summary

### Completed
âœ… Dataset reduced from 79 â†’ 7 columns (1900-2014 filtered)  
âœ… Kafka producer for streaming CSV data  
âœ… 2 Spark jobs: batch processing + streaming consumer  
âœ… PostgreSQL schema (7 columns, 3 tables, 3 views)  
âœ… All 5 components deployed to Kubernetes  
âœ… Minikube cluster running with all pods active  

### Simplifications Made
- Removed Docker Compose (Kubernetes-only deployment)
- Schema perfectly matches dataset (7 columns everywhere)
- Eliminated duplicate/redundant code (304 lines removed)
- Consolidated to 2 documentation files (README + STATUS)

### Current Phase
ï¿½ **Week 4**: Database initialization â†’ Data loading â†’ Dashboard creation

### Technical Specs
- **Deployment**: Kubernetes/Minikube (2 CPUs, 4GB RAM)
- **Images**: apache/kafka:4.1.0, apache/spark:4.0.1, postgres:latest
- **Data**: 23,405 rows Ã— 7 columns (1900-2014)
- **Storage**: 12Gi total PVCs (Kafka 5Gi, PostgreSQL 5Gi, Superset 2Gi)

---

## ğŸ‘¥ Project

**IACD** - Data Engineering Pipeline  
**Deployment**: Kubernetes  
**Status**: âœ… Data loaded & analyzed | ğŸ”„ Dashboards ready to create

---

## ğŸ“Š Quick Start Guide

### 1ï¸âƒ£ View Analysis Results
```bash
# See comprehensive findings
cat ANALYSIS.md
```
**Key Findings**:
- âœ… Global CO2 increased 17x from 1900 to 2024
- âœ… COVID caused -4.7% drop (2020) but fully recovered
- âœ… USA leads historically (425K Mt), China leads currently (12.3K Mt)
- âœ… China & India showed explosive growth (+237%, +224% since 2000)

### 2ï¸âƒ£ Access Superset Dashboards
```bash
# Superset is running and port-forwarded
open http://localhost:8088

# Login: admin / admin
```

**Follow these guides**:
- ğŸ“˜ **SUPERSET_SETUP.md** - Complete dashboard creation guide (12 charts)
- ğŸ“— **DASHBOARD_QUICK_START.md** - Quick reference for top 10 charts
- ğŸ“™ **sql/superset_queries.sql** - 12 categories of ready-to-use queries

### 3ï¸âƒ£ Query Data Directly
```bash
# PostgreSQL is running and port-forwarded
PGPASSWORD=co2_password psql -h localhost -U co2_user -d co2_data

# Example queries
SELECT * FROM top_polluters LIMIT 10;
SELECT * FROM recent_trends;
SELECT year, total_global_co2 FROM yearly_summary WHERE year >= 2018;
```

### 4ï¸âƒ£ Optional: Test Kafka Streaming
```bash
# Producer (streams CSV to Kafka)
python kafka/producer.py

# See STATUS.md for Spark consumer commands
```

---

## ğŸ“š Documentation Index

| Document | Purpose | Use When |
|----------|---------|----------|
| **README.md** (this file) | Project overview & architecture | First time setup |
| **STATUS.md** | Current deployment status | Checking what's running |
| **ANALYSIS.md** | Complete data insights | Presenting findings |
| **SUPERSET_SETUP.md** | Full dashboard guide (7 sections) | Creating visualizations |
| **DASHBOARD_QUICK_START.md** | Quick chart reference | Building specific charts |
| **sql/superset_queries.sql** | Pre-built SQL queries | Custom analysis |

---

## ğŸ¯ Current Status

âœ… **Infrastructure**: All 5 pods running (Kafka, Spark Master, Spark Worker, PostgreSQL, Superset)  
âœ… **Data Loaded**: 23,405 rows (1900-2024, 247 countries, 7 variables)  
âœ… **Tables Created**: raw_emissions, country_summary, yearly_summary + 3 views  
âœ… **Analysis Complete**: See `ANALYSIS.md` for findings  
âœ… **Port-Forwards Active**: PostgreSQL (5432), Superset (8088)  
ğŸ”„ **Dashboards**: Ready to create (follow `SUPERSET_SETUP.md`)  

See [`STATUS.md`](STATUS.md) for detailed deployment information.

---

**Last Updated**: November 16, 2025
