# ğŸ” LÃ³gica do Spark Consumer - ExplicaÃ§Ã£o Detalhada

## Fluxo Geral (VisÃ£o Macro)

```
Kafka (mensagens JSON) 
    â†“
Spark Consumer lÃª em batches (15 segundos)
    â†“
Limpa dados (remove NaNs, agregados)
    â†“
Agrupa por paÃ­s (mÃ©dia de CO2, GDP, etc)
    â†“
K-means Clustering (3 clusters)
    â†“
Guarda em PostgreSQL (co2_clusters, cluster_stats)
```

---

## Passo-a-Passo Detalhado

### 1ï¸âƒ£ **ConexÃ£o ao Kafka** (linhas 226-262)

```python
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "emissions-topic") \
    .option("startingOffsets", "earliest") \
    .load()
```

**O que faz:**
- Conecta ao Kafka (broker em `kafka:9092`)
- Subscreve tÃ³pico `emissions-topic`
- LÃª mensagens desde o inÃ­cio (`earliest`)

**Output:** Stream contÃ­nuo de mensagens

---

### 2ï¸âƒ£ **Parse JSON** (linhas 243-247)

```python
json_df = kafka_df.selectExpr("CAST(value AS STRING) as json_string")
processed_stream_df = json_df.select(
    from_json(col("json_string"), schema).alias("data")
).select("data.*")
```

**O que faz:**
- Converte bytes Kafka â†’ String
- Faz parse do JSON usando schema definido
- Extrai campos: country, year, iso_code, population, gdp, co2, co2_per_capita

**Exemplo de mensagem:**
```json
{
  "country": "Portugal",
  "year": 2020,
  "iso_code": "PRT",
  "population": 10196709,
  "gdp": 231049256960,
  "co2": 45.89,
  "co2_per_capita": 4.5
}
```

---

### 3ï¸âƒ£ **Limpeza de Dados** (linhas 59-87)

```python
# Converte "NaN" string â†’ NULL
for col_name in ["gdp", "population", "co2", "co2_per_capita"]:
    all_data = all_data.withColumn(
        col_name,
        when((col(col_name) == "NaN"), None).otherwise(col(col_name))
    )

# Filtra agregados (World, Europe, etc)
all_data = all_data.filter(
    (col("iso_code").isNotNull()) & 
    (col("iso_code") != "NaN")
)
```

**O que faz:**
- String "NaN" (do Pandas) â†’ NULL (do Spark)
- Remove registos onde `iso_code` Ã© NULL (agregados como "World", "Africa")

**PorquÃª?** 
- "World" nÃ£o Ã© um paÃ­s, Ã© soma de todos
- Queremos sÃ³ paÃ­ses individuais

---

### 4ï¸âƒ£ **AgregaÃ§Ã£o por PaÃ­s** (linhas 93-100)

```python
country_stats = all_data.groupBy("country", "iso_code").agg(
    avg("co2").alias("avg_co2"),
    avg("co2_per_capita").alias("avg_co2_per_capita"),
    avg("gdp").alias("avg_gdp"),
    avg("population").alias("avg_population"),
    count("*").alias("data_points")
).filter((col("avg_co2").isNotNull()) & (col("data_points") >= 5))
```

**O que faz:**
- Agrupa dados por **paÃ­s**
- Calcula **mÃ©dias** de todas as variÃ¡veis (CO2, GDP, populaÃ§Ã£o)
- Conta quantos registos (anos) cada paÃ­s tem
- SÃ³ mantÃ©m paÃ­ses com â‰¥5 anos de dados

**Exemplo de resultado:**
```
Country    | avg_co2 | avg_gdp       | avg_population
-----------|---------|---------------|---------------
Portugal   | 52.3    | 220000000000  | 10200000
Germany    | 850.2   | 3800000000000 | 83000000
```

---

### 5ï¸âƒ£ **PreparaÃ§Ã£o para Clustering** (linhas 116-126)

```python
feature_cols = ["avg_co2", "avg_co2_per_capita", "avg_gdp", "avg_population"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

cleaned_data = country_stats.na.drop()  # Remove NULLs

scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
```

**O que faz:**
- **VectorAssembler:** Junta as 4 colunas num vetor Ãºnico
  - `[52.3, 4.5, 220000000000, 10200000]` â†’ vetor
- **StandardScaler:** Normaliza valores (0-1)
  - PorquÃª? GDP Ã© muito maior que CO2, sem escalar o clustering ignora variÃ¡veis pequenas

**Antes:**
```
avg_gdp: 220000000000 (ENORME)
avg_co2: 52.3 (pequeno)
```

**Depois (scaled):**
```
gdp_scaled: 0.35
co2_scaled: 0.42
```

---

### 6ï¸âƒ£ **K-means Clustering** (linhas 128-143)

```python
k = 3  # 3 clusters
kmeans = KMeans(
    k=k, 
    featuresCol="scaled_features", 
    predictionCol="cluster",
    seed=42,
    maxIter=20
)

pipeline = Pipeline(stages=[assembler, scaler, kmeans])
model = pipeline.fit(cleaned_data)
results = model.transform(cleaned_data)
```

**O que faz:**
- **K-means:** Algoritmo de clustering (agrupa dados similares)
- **k=3:** Cria 3 grupos de paÃ­ses
- **seed=42:** Garante resultados reproduzÃ­veis
- **Pipeline:** Executa assembler â†’ scaler â†’ kmeans em sequÃªncia

**Resultado:** Cada paÃ­s recebe um `cluster` (0, 1 ou 2)

**Exemplo:**
```
Country    | avg_co2 | avg_gdp | cluster
-----------|---------|---------|--------
Portugal   | 52.3    | 2.2e11  | 1
USA        | 5000.0  | 2.1e13  | 2
Chad       | 2.5     | 1.1e10  | 0
```

**InterpretaÃ§Ã£o dos Clusters:**
- **Cluster 0:** Baixo CO2, baixo GDP (paÃ­ses em desenvolvimento)
- **Cluster 1:** MÃ©dio CO2, mÃ©dio GDP (paÃ­ses desenvolvidos mÃ©dios)
- **Cluster 2:** Alto CO2, alto GDP (paÃ­ses grandes/industrializados)

---

### 7ï¸âƒ£ **Guardar em PostgreSQL** (linhas 146-173)

```python
# Preparar dados
results_for_db = results.select(
    "country", "iso_code", "avg_co2", "avg_co2_per_capita", 
    "avg_gdp", "avg_population", "cluster"
).withColumn("batch_id", lit(batch_id))

# Guardar tabela principal
save_to_postgresql(results_for_db, batch_id, "co2_clusters")

# EstatÃ­sticas por cluster
cluster_stats = results.groupBy("cluster").agg(
    count("*").alias("num_countries"),
    avg("avg_co2").alias("avg_co2_cluster"),
    ...
)
save_to_postgresql(cluster_stats, batch_id, "cluster_stats")
```

**O que faz:**
- Seleciona colunas relevantes
- Adiciona `batch_id` (identificador do processamento)
- Guarda em 2 tabelas:
  1. **`co2_clusters`:** Cada paÃ­s com seu cluster
  2. **`cluster_stats`:** MÃ©dias por cluster

---

### 8ï¸âƒ£ **Clustering Temporal** (linhas 175-217) âš ï¸

**NOTA:** Esta parte dissemos que Ã© **desnecessÃ¡ria** e pode ser removida para simplificar!

Faz clustering por paÃ­s **E** ano (mais complexo, nÃ£o recomendado).

---

## ğŸ”„ Loop ContÃ­nuo (linha 272-277)

```python
query = kafka_stream.writeStream \
    .outputMode("update") \
    .foreachBatch(process_clustering) \
    .trigger(processingTime="15 seconds") \
    .start()
```

**O que faz:**
- A cada **15 segundos**, chama `process_clustering()`
- Processa batch de mensagens acumuladas
- Loop infinito (atÃ© Ctrl+C)

---

## ğŸ“Š Resumo Visual Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KAFKA (mensagens JSON de paÃ­ses)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Streaming (15s batches)
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SPARK CONSUMER                                          â”‚
â”‚                                                         â”‚
â”‚  1. Parse JSON         â†’ DataFrame                     â”‚
â”‚  2. Limpa NaNs         â†’ Remove agregados              â”‚
â”‚  3. Agrupa por paÃ­s    â†’ MÃ©dias (CO2, GDP, pop)        â”‚
â”‚  4. Normaliza          â†’ StandardScaler                â”‚
â”‚  5. K-means (k=3)      â†’ Atribui clusters 0/1/2        â”‚
â”‚  6. Guarda PostgreSQL  â†’ co2_clusters, cluster_stats   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POSTGRESQL                                              â”‚
â”‚  â”œâ”€ co2_clusters (paÃ­s + cluster)                      â”‚
â”‚  â””â”€ cluster_stats (mÃ©dias por cluster)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SUPERSET (lÃª e visualiza)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Conceitos Chave para Explicar

### **1. Streaming**
Em vez de processar tudo de uma vez, processa **continuamente** a cada 15s.

### **2. K-means**
Algoritmo que agrupa dados similares automaticamente (sem supervisÃ£o).

### **3. StandardScaler**
Normaliza valores para todas as variÃ¡veis terem o mesmo "peso" no clustering.

### **4. Pipeline**
SequÃªncia de transformaÃ§Ãµes (assembler â†’ scaler â†’ kmeans) executadas automaticamente.

---

**Esta Ã© a lÃ³gica completa do Consumer!** EstÃ¡ mais claro agora? ğŸ˜Š
